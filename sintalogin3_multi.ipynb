{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pickle\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_driver():\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    # chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def login_sinta(driver, username, password):\n",
    "    driver.get(\"https://sinta.kemdikbud.go.id/logins\")\n",
    "    time.sleep(3)\n",
    "    username_field = driver.find_element(By.NAME, \"username\") if len(driver.find_elements(By.NAME, \"username\")) > 0 else driver.find_element(By.XPATH, '//input[@name=\"username\"]')\n",
    "    password_field = driver.find_element(By.NAME, \"password\") if len(driver.find_elements(By.NAME, \"password\")) > 0 else driver.find_element(By.XPATH, '//input[@name=\"password\"]')\n",
    "    \n",
    "    username_field.send_keys(username)\n",
    "    password_field.send_keys(password)\n",
    "    \n",
    "    print(\"Username field value: \", username_field.get_attribute(\"value\"))\n",
    "    print(\"Password field value: \", password_field.get_attribute(\"value\"))\n",
    "    \n",
    "    login_button = driver.find_element(By.CSS_SELECTOR, 'button.btn.btn-block.btn-info')\n",
    "    login_button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    with open(\"sinta_session.pkl\", \"wb\") as session_file:\n",
    "        pickle.dump(driver.get_cookies(), session_file)\n",
    "    \n",
    "    print(\"Session saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_session(driver):\n",
    "    with open(\"sinta_session.pkl\", \"rb\") as session_file:\n",
    "        cookies = pickle.load(session_file)\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "    driver.refresh()\n",
    "    print(\"Session loaded successfully!\")\n",
    "\n",
    "def navigate_to_tab(driver, researcher_id, view):\n",
    "    driver.get(f\"https://sinta.kemdikbud.go.id/authors/profile/{researcher_id}/?view={view}\")\n",
    "    print(f\"Navigated to {view} tab for researcher profile with ID: {researcher_id}\")\n",
    "    time.sleep(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_researcher_info(driver):\n",
    "    try:\n",
    "        name = driver.find_element(By.CSS_SELECTOR, 'h3 > a').text\n",
    "    except:\n",
    "        name = \"N/A\"\n",
    "    try:\n",
    "        affiliation = driver.find_element(By.CSS_SELECTOR, 'a[href*=\"affiliations/profile\"]').text\n",
    "    except:\n",
    "        affiliation = \"N/A\"\n",
    "    try:\n",
    "        department = driver.find_element(By.CSS_SELECTOR, 'a[href*=\"departments/profile\"]').text\n",
    "    except:\n",
    "        department = \"N/A\"\n",
    "    \n",
    "    return {\n",
    "        \"Name\": name,\n",
    "        \"Affiliation\": affiliation,\n",
    "        \"Department\": department\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles(driver):\n",
    "    articles = []\n",
    "    try:\n",
    "        pagination_text_element = driver.find_element(By.CSS_SELECTOR, 'div.pagination-text')\n",
    "        pagination_text = pagination_text_element.text\n",
    "        match = re.search(r'Page \\d+ of (\\d+)', pagination_text)\n",
    "        if match:\n",
    "            total_pages = int(match.group(1))\n",
    "            print(f\"Total number of pages: {total_pages}\")\n",
    "        else:\n",
    "            total_pages = 1\n",
    "    except:\n",
    "        print(\"Pagination element not found, assuming only one page.\")\n",
    "        total_pages = 1\n",
    "\n",
    "    for page in range(1, total_pages + 1):\n",
    "        print(f\"Scraping page {page} of {total_pages}\")\n",
    "        time.sleep(3)\n",
    "        div_elements = driver.find_elements(By.CSS_SELECTOR, 'div.ar-list-item.mb-5')\n",
    "        for index, div in enumerate(div_elements):\n",
    "            print(f\"Scraping article {index + 1} on page {page}\")\n",
    "            try:\n",
    "                title = div.find_element(By.CSS_SELECTOR, 'div.ar-title > a').text\n",
    "            except:\n",
    "                title = \"N/A\"\n",
    "            try:\n",
    "                link = div.find_element(By.CSS_SELECTOR, 'div.ar-title > a').get_attribute(\"href\")\n",
    "            except:\n",
    "                link = \"N/A\"\n",
    "            try:\n",
    "                publisher = div.find_element(By.CSS_SELECTOR, 'div.ar-meta > a.ar-pub').text\n",
    "            except:\n",
    "                publisher = \"N/A\"\n",
    "            try:\n",
    "                journal = div.find_elements(By.CSS_SELECTOR, 'a.ar-pub')[-1].text if div.find_elements(By.CSS_SELECTOR, 'a.ar-pub') else \"N/A\"\n",
    "            except:\n",
    "                journal = \"N/A\"\n",
    "            try:\n",
    "                author_order = div.find_element(By.XPATH, '//a[contains(text(), \"Author Order\")]').text\n",
    "            except:\n",
    "                author_order = \"N/A\"\n",
    "            try:\n",
    "                if \"garuda\" in driver.current_url:\n",
    "                    authors = div.find_elements(By.CSS_SELECTOR, 'div.ar-meta > a')[1].text if len(div.find_elements(By.CSS_SELECTOR, 'div.ar-meta > a')) > 1 else \"N/A\"\n",
    "                elif \"googlescholar\" in driver.current_url:\n",
    "                    authors_div = div.find_element(By.CSS_SELECTOR, 'div.ar-meta')\n",
    "                    authors = authors_div.find_element(By.XPATH, './/a[contains(text(), \"Authors :\")]').text if \"Authors :\" in authors_div.text else \"N/A\"\n",
    "                else:\n",
    "                    authors_div = div.find_element(By.CSS_SELECTOR, 'div.ar-meta')\n",
    "                    authors = authors_div.find_element(By.XPATH, './/a[contains(text(), \"Authors :\")]').text if \"Authors :\" in authors_div.text else \"N/A\"\n",
    "            except:\n",
    "                authors = \"N/A\"\n",
    "            try:\n",
    "                year = div.find_element(By.CSS_SELECTOR, 'a.ar-year').text\n",
    "            except:\n",
    "                year = \"N/A\"\n",
    "            try:\n",
    "                doi = div.find_element(By.CSS_SELECTOR, 'a.ar-cited').text if \"DOI\" in div.find_element(By.CSS_SELECTOR, 'a.ar-cited').text else \"N/A\"\n",
    "            except:\n",
    "                doi = \"N/A\"\n",
    "            try:\n",
    "                quartile = div.find_element(By.CSS_SELECTOR, 'a.ar-quartile').text\n",
    "            except:\n",
    "                quartile = \"N/A\"\n",
    "            \n",
    "            articles.append({\n",
    "                \"Title\": title,\n",
    "                \"Link\": link,\n",
    "                \"Publisher\": publisher,\n",
    "                \"Journal\": journal,\n",
    "                \"Author Order\": author_order,\n",
    "                \"Authors\": authors,\n",
    "                \"Year\": year,\n",
    "                \"DOI\": doi,\n",
    "                \"Quartile\": quartile\n",
    "            })\n",
    "        \n",
    "        if page < total_pages:\n",
    "            try:\n",
    "                next_button = driver.find_element(By.LINK_TEXT, \"Next\")\n",
    "                next_button.click()\n",
    "            except:\n",
    "                print(\"No next button found, stopping navigation.\")\n",
    "                break\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_articles_to_files(articles, researcher_info, researcher_id, filename_prefix):\n",
    "    articles_df = pd.DataFrame(articles)\n",
    "    articles_df[\"Researcher Name\"] = researcher_info[\"Name\"]\n",
    "    articles_df[\"Researcher ID\"] = researcher_id\n",
    "    articles_df[\"Affiliation\"] = researcher_info[\"Affiliation\"]\n",
    "    articles_df[\"Department\"] = researcher_info[\"Department\"]\n",
    "    articles_df.to_csv(f\"{filename_prefix}_articles.csv\", index=False)\n",
    "    articles_df.to_excel(f\"{filename_prefix}_articles.xlsx\", index=False)\n",
    "    print(articles_df)\n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_statistics(scopus_df, wos_df, garuda_df, googlescholar_df, researcher_id):\n",
    "    # Combine all dataframes into one\n",
    "    combined_df = pd.concat([scopus_df, wos_df, garuda_df, googlescholar_df], keys=['Scopus', 'WOS', 'Garuda', 'GoogleScholar'], names=['Source'])\n",
    "    \n",
    "    # General statistics\n",
    "    total_articles = combined_df.shape[0]\n",
    "    articles_per_source = combined_df.groupby(level='Source').size()\n",
    "    articles_per_year = combined_df['Year'].value_counts()\n",
    "    \n",
    "    # Convert statistics to DataFrame\n",
    "    stats_df = pd.DataFrame({\n",
    "        \"Source\": articles_per_source.index,\n",
    "        \"Articles Count\": articles_per_source.values\n",
    "    })\n",
    "    stats_df_year = articles_per_year.reset_index()\n",
    "    stats_df_year.columns = [\"Year\", \"Articles Count\"]\n",
    "    \n",
    "    # Save statistics to CSV\n",
    "    stats_df.to_csv(f\"statistics_summary_source_{researcher_id}.csv\", index=False)\n",
    "    stats_df_year.to_csv(f\"statistics_summary_year_{researcher_id}.csv\", index=False)\n",
    "    \n",
    "    print(\"Total number of articles for researcher_id\", researcher_id, \":\", total_articles)\n",
    "    print(\"Number of articles per source:\")\n",
    "    print(stats_df)\n",
    "    print(\"Number of articles per year:\")\n",
    "    print(stats_df_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    username = \"\"\n",
    "    password = \"\"\n",
    "    researcher_ids = [\"5986966\",\"161222\", \"6172418\"]  # Add multiple researcher IDs here\n",
    "    \n",
    "    import pandas as pd\n",
    "\n",
    "    # Load the dataset with researcher information\n",
    "    file_path = 'authors_data_teknikindustri.csv'\n",
    "    authors_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Extract the necessary columns: SINTA_ID, Author_Name, University, Department\n",
    "    researcher_data = authors_df[['SINTA_ID', 'Author_Name', 'University', 'Department']]\n",
    "\n",
    "    # Prepare the list of researcher IDs and related information for the scraping script\n",
    "    researcher_ids = researcher_data['SINTA_ID'].tolist()\n",
    "    researcher_names = researcher_data['Author_Name'].tolist()\n",
    "    universities = researcher_data['University'].tolist()\n",
    "    departments = researcher_data['Department'].tolist()\n",
    "        \n",
    "    driver = setup_driver()\n",
    "    try:\n",
    "        login_sinta(driver, username, password)\n",
    "        load_session(driver)\n",
    "        \n",
    "        for index, researcher_id in enumerate(researcher_ids):\n",
    "            researcher_name = researcher_names[index]\n",
    "            university = universities[index]\n",
    "            department = departments[index]\n",
    "            researcher_info = {\n",
    "                \"Name\": researcher_name,\n",
    "                \"Affiliation\": university,\n",
    "                \"Department\": department\n",
    "            }\n",
    "            print(f\"Starting scraping for researcher ID: {researcher_id}\")\n",
    "            # Scrape researcher information\n",
    "            navigate_to_tab(driver, researcher_id, \"\")\n",
    "            researcher_info = scrape_researcher_info(driver)\n",
    "            \n",
    "            # Scrape Scopus Articles\n",
    "            navigate_to_tab(driver, researcher_id, \"scopus\")\n",
    "            scopus_articles = scrape_articles(driver)\n",
    "            scopus_df = save_articles_to_files(scopus_articles, researcher_info, researcher_id, f\"scopus_{researcher_id}\")\n",
    "            \n",
    "            # Scrape WOS Articles\n",
    "            navigate_to_tab(driver, researcher_id, \"wos\")\n",
    "            wos_articles = scrape_articles(driver)\n",
    "            wos_df = save_articles_to_files(wos_articles, researcher_info, researcher_id, f\"wos_{researcher_id}\")\n",
    "            \n",
    "            # Scrape Garuda Articles\n",
    "            navigate_to_tab(driver, researcher_id, \"garuda\")\n",
    "            garuda_articles = scrape_articles(driver)\n",
    "            garuda_df = save_articles_to_files(garuda_articles, researcher_info, researcher_id, f\"garuda_{researcher_id}\")\n",
    "            \n",
    "            # Scrape Google Scholar Articles\n",
    "            navigate_to_tab(driver, researcher_id, \"googlescholar\")\n",
    "            googlescholar_articles = scrape_articles(driver)\n",
    "            googlescholar_df = save_articles_to_files(googlescholar_articles, researcher_info, researcher_id, f\"googlescholar_{researcher_id}\")\n",
    "            \n",
    "            # Generate statistics from all sources for each researcher\n",
    "            generate_statistics(scopus_df, wos_df, garuda_df, googlescholar_df, researcher_id)\n",
    "            print(f\"Finished scraping for researcher ID: {researcher_id}\\n\")\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
